{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27baad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Libraries ##\n",
    "###############\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bd460",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48538099",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## Load in MNIST Digits ##\n",
    "##########################\n",
    "\n",
    "all_data = np.load(\"/scratch/gpfs/eysu/src_data/mnist.npz\")\n",
    "print(all_data.files)\n",
    "x_test = all_data['x_test']\n",
    "x_train = all_data['x_train']\n",
    "y_train = all_data['y_train']\n",
    "y_test = all_data['y_test']\n",
    "\n",
    "print(x_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## Partition and resize data ##\n",
    "###############################\n",
    "\n",
    "labels = [\"0\",  # index 0\n",
    "          \"1\",  # index 1\n",
    "          \"2\",  # index 2 \n",
    "          \"3\",  # index 3 \n",
    "          \"4\",  # index 4\n",
    "          \"5\",  # index 5\n",
    "          \"6\",  # index 6 \n",
    "          \"7\",  # index 7 \n",
    "          \"8\",  # index 8 \n",
    "          \"9\"]  # index 9\n",
    "\n",
    "# save train labels\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train_labels = y_train\n",
    "y_test_labels = y_test\n",
    "\n",
    "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
    "\n",
    "# Validation set\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e689fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Load, partition, and resize MNIST Digits ##\n",
    "##############################################\n",
    "\n",
    "## SAME THING JUST AS A FUNCTION\n",
    "def loadData():\n",
    "    all_data = np.load(\"/scratch/gpfs/eysu/src_data/mnist.npz\")\n",
    "\n",
    "    x_test = all_data['x_test']\n",
    "    x_train = all_data['x_train']\n",
    "    y_train = all_data['y_train']\n",
    "    y_test = all_data['y_test']\n",
    "\n",
    "    labels = [\"0\",  # index 0\n",
    "              \"1\",  # index 1\n",
    "              \"2\",  # index 2 \n",
    "              \"3\",  # index 3 \n",
    "              \"4\",  # index 4\n",
    "              \"5\",  # index 5\n",
    "              \"6\",  # index 6 \n",
    "              \"7\",  # index 7 \n",
    "              \"8\",  # index 8 \n",
    "              \"9\"]  # index 9\n",
    "\n",
    "    # save train labels\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "\n",
    "    # y_train_labels = y_train\n",
    "    # y_test_labels = y_test\n",
    "\n",
    "    # Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
    "    (x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
    "    (y_train, y_valid) = y_train[5000:], np.array(y_train[:5000]).squeeze()\n",
    "\n",
    "    # Reshape input data from (28, 28) to (28, 28, 1)\n",
    "    w, h = 28, 28\n",
    "    x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
    "    \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eccfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine any image\n",
    "\n",
    "# Image index, you can pick any number between 0 and 44,999\n",
    "img_index = 0\n",
    "label_index = y_train[img_index]\n",
    "# Print the label, for example 2 Pullover\n",
    "print(\"y = \" + str(label_index) + \" (\" +(labels[label_index]) + \")\")\n",
    "plt.imshow(x_train[img_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3893c9",
   "metadata": {},
   "source": [
    "# Iterated Retraining By Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Sampling Helper Function ##\n",
    "##############################\n",
    "\n",
    "def sample(distributions):\n",
    "    N = distributions.shape[0]\n",
    "    labels = [None] * N\n",
    "    for i in range(N):\n",
    "        label = np.random.choice(10, p=distributions[i])\n",
    "        labels[i] = label\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efce5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## This cell runs the iterated learning training procedure. ##\n",
    "##############################################################\n",
    "for END_IDX in range(5, 6):\n",
    "    # Number of iterations in the serial reproduction\n",
    "    MAX_ITER = 1000\n",
    "    # Number of epochs per training run\n",
    "    EPOCHS = 10\n",
    "     \n",
    "    save_path = \"/scratch/gpfs/eysu/Sampling/pretrained2_\" + str(MAX_ITER) + \"/\"\n",
    "    weight_path = \"/scratch/gpfs/eysu/low_shot_weights/\" + str(END_IDX) + \"/\"\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = loadData()\n",
    "\n",
    "    # create an empty array to store the new labels for every iter\n",
    "    all_labels = np.zeros((x_train.shape[0], MAX_ITER + 1))\n",
    "\n",
    "    for iteration in range(0,MAX_ITER):\n",
    "        # If iteration is seed, train on original target vectors, else, train on y_hat from time t-1\n",
    "        if iteration == 0:\n",
    "            # Save the label and then one-hot encode the labels\n",
    "            all_labels[:, 0] = y_train\n",
    "            y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "            y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "            y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "            \n",
    "            mpth = 'model.weights.best.hdf5'\n",
    "            y_hat_test_name = 'y_hat_test_seed'\n",
    "            y_hat_train_name = 'y_hat_train_seed'      \n",
    "        elif iteration > 0:\n",
    "            # Key step: set new targets as y_hat\n",
    "            y_train = new_train\n",
    "            mpth = 'model.weights.best.' + 'iter' + str(iteration) + '.hdf5'\n",
    "            y_hat_test_name = 'y_hat_test_' + 'iter' + str(iteration)\n",
    "            y_hat_train_name = 'y_hat_train_' + 'iter' + str(iteration)\n",
    "\n",
    "        # Define the model: a small CNN model (could probably be done outside loop)\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # Must define the input shape in the first layer of the neural network\n",
    "        model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        # Each time, use the pretrained model with the prior from the lo shot training\n",
    "        model.load_weights(weight_path + 'model.weights.best.pretrain.hdf5')\n",
    "        # model.summary()\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # Save checkpoints\n",
    "        checkpointer = ModelCheckpoint(filepath= save_path + mpth, verbose = 1, save_best_only=True) #True\n",
    "        # Train the model\n",
    "        model.fit(x_train,\n",
    "                 y_train,\n",
    "                 batch_size=64,\n",
    "                 epochs=EPOCHS,\n",
    "                 validation_data=(x_valid, y_valid),\n",
    "                 callbacks=[checkpointer])\n",
    "\n",
    "        # Load the weights with the best validation accuracy\n",
    "        y_hat = model.predict(x_train) #feed back serial reproduction targets\n",
    "        y_hat_test = model.predict(x_test)\n",
    "\n",
    "        #### START OF SAMPLING ####\n",
    "\n",
    "        # use helper function to sample label for every image in train \n",
    "        new_labels = np.array(sample(y_hat))\n",
    "\n",
    "        # store new labels for all images under its corresponding iteration\n",
    "        all_labels[:, iteration + 1] = new_labels\n",
    "\n",
    "        # expand dimensions of new labels and set this as new training vector\n",
    "        new_train = tf.keras.utils.to_categorical(new_labels, 10)\n",
    "\n",
    "        #### END OF SAMPLING ####\n",
    "\n",
    "        model.load_weights(save_path + mpth)\n",
    "        # Evaluate the model on test set\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        # Print test accuracy\n",
    "        print('\\n', 'Test accuracy:', score[1])\n",
    "\n",
    "        # Save results for each iteration in the serial reproduction chain\n",
    "        np.save(save_path + y_hat_train_name + '.npy', y_train)\n",
    "        print(save_path + y_hat_train_name)\n",
    "\n",
    "        np.save(save_path + y_hat_test_name + '.npy', y_hat_test)\n",
    "        print(save_path + y_hat_test_name)\n",
    "\n",
    "    np.save(save_path + 'labels.npy', all_labels)\n",
    "    print('Saved labels!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fe1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0290b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c039b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91debe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b313852b",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d5066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff38f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu [~/.conda/envs/tf2-gpu/]",
   "language": "python",
   "name": "conda_tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
