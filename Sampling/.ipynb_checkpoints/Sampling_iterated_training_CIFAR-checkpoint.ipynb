{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27baad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Libraries ##\n",
    "###############\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bd460",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48538099",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Import CIFAR10 data from Scratch ##\n",
    "######################################\n",
    "import pickle\n",
    "\n",
    "# unpickle the binary files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# paths to each batch of data\n",
    "batch1 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_1\")\n",
    "batch2 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_2\")\n",
    "batch3 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_3\")\n",
    "batch4 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_4\")\n",
    "batch5 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_5\")\n",
    "meta = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/batches.meta\")\n",
    "test = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/test_batch\")\n",
    "\n",
    "# separate labels and image data from each batch\n",
    "y_train1 = batch1[b'labels']\n",
    "x_train1 = batch1[b'data']\n",
    "y_train2 = batch2[b'labels']\n",
    "x_train2 = batch2[b'data']\n",
    "y_train3 = batch3[b'labels']\n",
    "x_train3 = batch3[b'data']\n",
    "y_train4 = batch4[b'labels']\n",
    "x_train4 = batch4[b'data']\n",
    "y_train5 = batch5[b'labels']\n",
    "x_train5 = batch5[b'data']\n",
    "\n",
    "# concatenate into big training and testing arrays\n",
    "y_train = np.concatenate((y_train1, y_train2, y_train3, y_train4, y_train5))\n",
    "x_train = np.concatenate((x_train1, x_train2, x_train3, x_train4, x_train5), axis=0)\n",
    "\n",
    "# def shuffle_in_unison(x, y):\n",
    "#     assert x.shape[0] == y.shape[0]\n",
    "#     shuffled_x = np.empty(x.shape, dtype=x.dtype)\n",
    "#     shuffled_y = np.empty(y.shape, dtype=y.dtype)\n",
    "    \n",
    "#     # If rerunning: \n",
    "#     # permutation = np.loadtxt('/scratch/gpfs/eysu/src_data/cifar-10-batches-py/permutation.csv', delimiter=',').astype(np.int64)\n",
    "    \n",
    "#     # If repermuting: \n",
    "#     # permutation = np.random.permutation(y.shape[0])\n",
    "    \n",
    "#     for old_index, new_index in enumerate(permutation):\n",
    "#         shuffled_x[new_index] = x[old_index]\n",
    "#         shuffled_y[new_index] = y[old_index]\n",
    "\n",
    "#     # IF I WANT TO RUN THIS AGAIN DONT REPERMUTE OR WILL LOSE THIS ORDERING\n",
    "# #     np.savetxt('/scratch/gpfs/eysu/src_data/cifar-10-batches-py/permutation.csv', permutation, delimiter=',')\n",
    "#     return shuffled_x, shuffled_y\n",
    "\n",
    "# x_train, y_train = shuffle_in_unison(x_train, y_train)\n",
    "\n",
    "y_test = test[b'labels']\n",
    "x_test = test[b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "## Preprocess data by reshaping and separating ##\n",
    "#################################################\n",
    "labels = ['airplane',  # index 0\n",
    "          'automobile',  # index 1\n",
    "          'bird',  # index 2 \n",
    "          'cat',  # index 3 \n",
    "          'deer',  # index 4\n",
    "          'dog',  # index 5\n",
    "          'frog',  # index 6 \n",
    "          'horse',  # index 7 \n",
    "          'ship',  # index 8 \n",
    "          'truck']  # index 9\n",
    "\n",
    "# Further break training data into train / validation sets \n",
    "# put 5000 into validation set and keep remaining 45,000 for train\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "# reshape data to match dimensions of cifar10.load_data\n",
    "x_train = x_train.reshape(45000, 3, 32, 32)\n",
    "x_train = x_train.transpose(0, 2, 3, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "\n",
    "x_valid = x_valid.reshape(5000, 3, 32, 32)\n",
    "x_valid = x_valid.transpose(0, 2, 3, 1)\n",
    "x_valid = x_valid.astype('float32')\n",
    "x_valid /= 255\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "\n",
    "x_test = x_test.reshape(10000, 3, 32, 32)\n",
    "x_test = x_test.transpose(0, 2, 3, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "# assert dimensions of data\n",
    "print(\"TRAINING DATA\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"VALIDATION DATA\")\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(\"TESTING DATA\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eccfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine any image\n",
    "\n",
    "# Image index, you can pick any number between 0 and 44,999\n",
    "img_index = 35\n",
    "label_index = y_train[img_index]\n",
    "# Print the label, for example 2 Pullover\n",
    "print(\"y = \" + str(label_index) + \" (\" +(labels[label_index]) + \")\")\n",
    "plt.imshow(x_train[img_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3893c9",
   "metadata": {},
   "source": [
    "# Iterated Retraining By Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Sampling Helper Function ##\n",
    "##############################\n",
    "\n",
    "def sample(distributions):\n",
    "    N = distributions.shape[0]\n",
    "    labels = [None] * N\n",
    "    for i in range(N):\n",
    "        label = np.random.choice(10, p=distributions[i])\n",
    "        labels[i] = label\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## This cell runs the iterated learning training procedure. ##\n",
    "##############################################################\n",
    "\n",
    "# Number of iterations in the serial reproduction\n",
    "MAX_ITER = 1000\n",
    "# Number of epochs per training run\n",
    "EPOCHS = 10\n",
    "\n",
    "# create an empty array to store the new labels for every iter\n",
    "all_labels = np.zeros((x_train.shape[0], MAX_ITER + 1))\n",
    "test_labels = np.zeros((x_test.shape[0], MAX_ITER + 1))\n",
    "\n",
    "for iteration in range(0,MAX_ITER):\n",
    "    # If iteration is seed, train on original target vectors, else, train on y_hat from time t-1\n",
    "    if iteration == 0:\n",
    "        # Save the label and then one-hot encode the labels\n",
    "        all_labels[:, 0] = y_train\n",
    "        test_labels[:, 0] = y_test\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "        mpth = 'model.weights.best.hdf5'\n",
    "        y_hat_test_name = 'y_hat_test_seed'\n",
    "        y_hat_train_name = 'y_hat_train_seed'      \n",
    "    elif iteration > 0:\n",
    "        # Key step: set new targets as y_hat\n",
    "        y_train = new_train\n",
    "        mpth = 'model.weights.best.' + 'iter' + str(iteration) + '.hdf5'\n",
    "        y_hat_test_name = 'y_hat_test_' + 'iter' + str(iteration)\n",
    "        y_hat_train_name = 'y_hat_train_' + 'iter' + str(iteration)\n",
    "\n",
    "    # Define the model: a small CNN model (could probably be done outside loop)\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Must define the input shape in the first layer of the neural network\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(32,32,3))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Take a look at the model summary\n",
    "    # model.summary()\n",
    "\n",
    "    # define optimization and energy parameters\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Save checkpoints\n",
    "    checkpointer = ModelCheckpoint(filepath= '/scratch/gpfs/eysu/Sampling/CIFAR2_1000/' + mpth, verbose = 1, save_best_only=True) #True\n",
    "    # Train the model\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=EPOCHS,\n",
    "             validation_data=(x_valid, y_valid),\n",
    "             callbacks=[checkpointer])\n",
    "\n",
    "    # Load the weights with the best validation accuracy\n",
    "    y_hat = model.predict(x_train) #feed back serial reproduction targets\n",
    "    y_hat_test = model.predict(x_test)\n",
    "    \n",
    "    #### START OF SAMPLING ####\n",
    "\n",
    "    # use helper function to sample label for every image in train \n",
    "    new_labels = np.array(sample(y_hat))\n",
    "    new_test_labels = np.array(sample(y_hat_test))\n",
    "    \n",
    "    # store new labels for all images under its corresponding iteration\n",
    "    all_labels[:, iteration + 1] = new_labels\n",
    "    test_labels[:, iteration + 1] = new_test_labels\n",
    "    # expand dimensions of new labels and set this as new training vector\n",
    "    new_train = tf.keras.utils.to_categorical(new_labels, 10)\n",
    "    \n",
    "    #### END OF SAMPLING ####\n",
    "    \n",
    "    model.load_weights('/scratch/gpfs/eysu/Sampling/CIFAR2_1000/' + mpth)\n",
    "    # Evaluate the model on test set\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    # Print test accuracy\n",
    "    print('\\n', 'Test accuracy:', score[1])\n",
    "\n",
    "    # Save results for each iteration in the serial reproduction chain\n",
    "    np.save('/scratch/gpfs/eysu/Sampling/CIFAR2_1000/' + y_hat_train_name + '.npy', y_train)\n",
    "    print('/scratch/gpfs/eysu/Sampling/CIFAR2_1000/' + y_hat_train_name)\n",
    "\n",
    "    np.save('/scratch/gpfs/eysu/Sampling/CIFAR2_1000/' + y_hat_test_name + '.npy', y_hat_test)\n",
    "    print('/scratch/gpfs/eysu/Sampling/CIFAR2_1000/' + y_hat_test_name)\n",
    "\n",
    "    \n",
    "np.save('/scratch/gpfs/eysu/Sampling/CIFAR2_1000/labels.npy', all_labels)\n",
    "np.save('/scratch/gpfs/eysu/Sampling/CIFAR2_1000/test_labels.npy', test_labels)\n",
    "print('Saved labels!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0be02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a5a28b",
   "metadata": {},
   "source": [
    "# Analyze Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25309d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "## Look at divisions in labels by class ##\n",
    "##########################################\n",
    "\n",
    "labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_1000/labels.npy')\n",
    "print(labels.shape)\n",
    "# print(labels)\n",
    "\n",
    "# for i in range(201):\n",
    "#     print(np.unique(labels[:, i], return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8ac25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2778bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Examine the divisions in classes across iterations ##\n",
    "########################################################\n",
    "all_labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_200_1/labels.npy')\n",
    "\n",
    "# store all divisions of classes in array\n",
    "# dimensions are iters x classes\n",
    "divisions = np.zeros((all_labels.shape[1], 10))\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    _, counts = np.unique(all_labels[:, i], return_counts=True)\n",
    "    divisions[i] = counts\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for j in range(10):\n",
    "    x = np.arange(all_labels.shape[1])\n",
    "    plt.plot(x, divisions[:, j], label = \"Class: \" + str(j))\n",
    "    \n",
    "plt.title(\"Divisions of classes over 200 iterations - Run 1\")\n",
    "plt.ylim([0, 12000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2687e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Examine the divisions in classes across iterations ##\n",
    "########################################################\n",
    "all_labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_200_2/labels.npy')\n",
    "\n",
    "# store all divisions of classes in array\n",
    "# dimensions are iters x classes\n",
    "divisions = np.zeros((all_labels.shape[1], 10))\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    _, counts = np.unique(all_labels[:, i], return_counts=True)\n",
    "    divisions[i] = counts\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for j in range(10):\n",
    "    x = np.arange(all_labels.shape[1])\n",
    "    plt.plot(x, divisions[:, j], label = \"Class: \" + str(j))\n",
    "    \n",
    "plt.title(\"Divisions of classes over 200 iterations - Run 2\")\n",
    "plt.ylim([0, 12000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Examine the divisions in classes across iterations ##\n",
    "########################################################\n",
    "all_labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_200_3/labels.npy')\n",
    "\n",
    "# store all divisions of classes in array\n",
    "# dimensions are iters x classes\n",
    "divisions = np.zeros((all_labels.shape[1], 10))\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    _, counts = np.unique(all_labels[:, i], return_counts=True)\n",
    "    divisions[i] = counts\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for j in range(10):\n",
    "    x = np.arange(all_labels.shape[1])\n",
    "    plt.plot(x, divisions[:, j], label = \"Class: \" + str(j))\n",
    "    \n",
    "plt.title(\"Divisions of classes over 200 iterations - Run 3\")\n",
    "plt.ylim([0, 12000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Examine the divisions in classes across iterations ##\n",
    "########################################################\n",
    "all_labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_200_4/labels.npy')\n",
    "\n",
    "# store all divisions of classes in array\n",
    "# dimensions are iters x classes\n",
    "divisions = np.zeros((all_labels.shape[1], 10))\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    _, counts = np.unique(all_labels[:, i], return_counts=True)\n",
    "    divisions[i] = counts\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for j in range(10):\n",
    "    x = np.arange(all_labels.shape[1])\n",
    "    plt.plot(x, divisions[:, j], label = \"Class: \" + str(j))\n",
    "    \n",
    "plt.title(\"Divisions of classes over 200 iterations - Run 4\")\n",
    "plt.ylim([0, 12000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdc506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Examine the divisions in classes across iterations ##\n",
    "########################################################\n",
    "all_labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_200_5/labels.npy')\n",
    "\n",
    "# store all divisions of classes in array\n",
    "# dimensions are iters x classes\n",
    "divisions = np.zeros((all_labels.shape[1], 10))\n",
    "\n",
    "for i in range(all_labels.shape[1]):\n",
    "    _, counts = np.unique(all_labels[:, i], return_counts=True)\n",
    "    divisions[i] = counts\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for j in range(10):\n",
    "    x = np.arange(all_labels.shape[1])\n",
    "    plt.plot(x, divisions[:, j], label = \"Class: \" + str(j))\n",
    "    \n",
    "plt.title(\"Divisions of classes over 200 iterations - Run 5\")\n",
    "plt.ylim([0, 12000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fb0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de70a9b2",
   "metadata": {},
   "source": [
    "# Look at specific samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_200_1/labels.npy')\n",
    "\n",
    "for i in range(10):\n",
    "    pdf = PdfPages('/home/eysu/Sampling/Outputs/Class_' + str(i) + '.pdf')\n",
    "    for j in range(100):\n",
    "        # isolate the labels that start off as each index\n",
    "        # plot the first example \n",
    "        x = np.arange(labels.shape[1])\n",
    "        class_labels = labels[labels[:, 0] == i]\n",
    "\n",
    "        # print all class labels for random image in class i\n",
    "        rand_idx = np.random.randint(0, class_labels.shape[0])\n",
    "        fig = plt.figure()\n",
    "        plt.plot(x, class_labels[rand_idx, :])\n",
    "        plt.ylim(-1, 10)\n",
    "        plt.title('Class: ' + str(i))\n",
    "        plt.show()\n",
    "        \n",
    "        # save all to pdf\n",
    "        pdf.savefig(fig, bbox_inches = 'tight')\n",
    "    pdf.close()\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2173cb",
   "metadata": {},
   "source": [
    "# Class Images Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "## Look at how image examples in each class change ##\n",
    "#####################################################\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "labels = np.load('/scratch/gpfs/eysu/Sampling/MNIST_1000/labels.npy')\n",
    "CLASS = 0\n",
    " \n",
    "for iter in [0, 200, 400, 600, 800, 1000]:\n",
    "    pdf = PdfPages('/home/eysu/Sampling/Outputs/Class_images_' + str(CLASS) + '_iter_' + str(iter) + '.pdf')\n",
    "\n",
    "    # find indices of images labeled with CLASS\n",
    "    class_idxs = np.where(labels[:, iter] == CLASS)\n",
    "    # class images\n",
    "    class_imgs = x_train[class_idxs]\n",
    "    # class labels\n",
    "    class_labels = labels[class_idxs]\n",
    "\n",
    "    # make sure class_labels only includes CLASS\n",
    "    assert np.unique(class_labels[:, iter])[0] == CLASS\n",
    "\n",
    "    for i in range(class_labels.shape[0]):\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(class_imgs[i])\n",
    "        plt.show()\n",
    "\n",
    "        # save all to pdf\n",
    "        pdf.savefig(fig, bbox_inches = 'tight')\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS = 0\n",
    "# find indices of images labeled with CLASS\n",
    "class_idxs = np.where(labels[:, 1000] == CLASS)\n",
    "# class images\n",
    "class_imgs = x_train[class_idxs]\n",
    "# class labels\n",
    "class_labels = labels[class_idxs]\n",
    "\n",
    "x = [0, 200, 400, 600, 800, 1000]\n",
    "counts = np.zeros((len(x), 10))\n",
    "for i, iter in enumerate(x):\n",
    "    classes, iter_counts = np.unique(class_labels[:, iter], return_counts = True)\n",
    "    \n",
    "    # if all labels are the same only update one index\n",
    "    if len(classes) == 1:\n",
    "        counts[i, int(classes[0])] = iter_counts[0]\n",
    "    \n",
    "    else: counts[i] = iter_counts\n",
    "    \n",
    "print(counts)\n",
    "    \n",
    "for j in range(10):\n",
    "    plt.scatter(x, counts[:, j], label = \"class: \" + str(j))\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Distribution of classes at each iteration for images with final label=0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef611cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS = 0\n",
    "# find indices of images labeled with CLASS\n",
    "class_idxs = np.where(labels[:, 1000] == CLASS)\n",
    "# class images\n",
    "class_imgs = x_train[class_idxs]\n",
    "# class labels\n",
    "class_labels = labels[class_idxs]\n",
    "\n",
    "x = np.arange(800, 1001)\n",
    "counts = np.zeros((len(x), 10))\n",
    "for i, iter in enumerate(x):\n",
    "    classes, iter_counts = np.unique(class_labels[:, iter], return_counts = True)\n",
    "    \n",
    "    # if all labels are the same only update one index\n",
    "    if len(classes) == 1:\n",
    "        counts[i, int(classes[0])] = iter_counts[0]\n",
    "    else: counts[i] = iter_counts\n",
    "    \n",
    "print(counts)\n",
    "    \n",
    "for j in range(10):\n",
    "    plt.scatter(x, counts[:, j], label = \"class: \" + str(j))\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Distribution of classes at each iteration for images with final label=0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc522cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just print the distribution before the very last one\n",
    "x = [999, 999, 999, 999, 999, 999, 999, 999, 999, 999]\n",
    "\n",
    "y = counts[-2]\n",
    "plt.scatter(x, counts[-2])\n",
    "x = x + 1\n",
    "plt.scatter(x, counts[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10cc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ffd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb1379e7",
   "metadata": {},
   "source": [
    "# Look at fraction of images remaining in their true classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abdf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [200, 400, 600, 800, 1000]\n",
    "true = labels[:, 0]\n",
    "for iter in x:\n",
    "    final = labels[:, iter]\n",
    "\n",
    "    remain = 1 - (np.count_nonzero((true - final).astype(int)) / labels.shape[0])\n",
    "    print(\"iter \" + str(iter) + \": \" + str(remain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c5251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8da4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b313852b",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d5066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff38f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu [~/.conda/envs/tf2-gpu/]",
   "language": "python",
   "name": "conda_tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
