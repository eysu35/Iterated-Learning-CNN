{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Libraries ##\n",
    "###############\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "\n",
    "import saliency\n",
    "from matplotlib import cm\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "## This cell is for selecting the dataset --- Digits or Fashion (MNIST toy world) ##\n",
    "####################################################################################\n",
    "\n",
    "DATASET = '_DigitMNIST' \n",
    "# DATASET = '_FashionMNIST'\n",
    "REGIME = '_TRAINED_' # '_RANDOM_'\n",
    "\n",
    "if DATASET == '_DigitMNIST':\n",
    "    # Load the digit-mnist pre-shuffled train data and test data\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() #digit_mnist\n",
    "    print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "\n",
    "    # Define the text labels\n",
    "    labels = [\"0\",  # index 0\n",
    "                            \"1\",  # index 1\n",
    "                            \"2\",  # index 2 \n",
    "                            \"3\",  # index 3 \n",
    "                            \"4\",  # index 4\n",
    "                            \"5\",  # index 5\n",
    "                            \"6\",  # index 6 \n",
    "                            \"7\",  # index 7 \n",
    "                            \"8\",  # index 8 \n",
    "                            \"9\"]  # index 9\n",
    "\n",
    "else:\n",
    "    # Load the fashion-mnist pre-shuffled train data and test data\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() #fashion_mnist\n",
    "    print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "\n",
    "    # Define the text labels\n",
    "    labels = [\"T-shirt\",  # index 0\n",
    "                            \"Trouser\",      # index 1\n",
    "                            \"Pullover\",     # index 2 \n",
    "                            \"Dress\",        # index 3 \n",
    "                            \"Coat\",         # index 4\n",
    "                            \"Sandal\",       # index 5\n",
    "                            \"Shirt\",        # index 6 \n",
    "                            \"Sneaker\",      # index 7 \n",
    "                            \"Bag\",          # index 8 \n",
    "                            \"Ankle boot\"]   # index 9\n",
    "\n",
    "# Print training set shape - note there are 60,000 training data of image size of 28x28, 60,000 train labels)\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "\n",
    "# save train labels\n",
    "y_train_labels = y_train\n",
    "y_test_labels = y_test\n",
    "\n",
    "# Print the number of training and test datasets\n",
    "print(x_train.shape[0], 'train set')\n",
    "print(x_test.shape[0], 'test set')\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
    "\n",
    "# Validation set\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "# Image index, you can pick any number between 0 and 59,999\n",
    "img_index = 5\n",
    "# y_train contains the lables, ranging from 0 to 9\n",
    "label_index = y_train[img_index]\n",
    "# Print the label, for example 2 Pullover\n",
    "print (\"y = \" + str(label_index) + \" \" +(labels[label_index]))\n",
    "# # Show one of the images from the training dataset\n",
    "plt.imshow(x_train[img_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b1292",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "## This cell runs the serial reproduction training procedure. ##\n",
    "################################################################\n",
    "\n",
    "# Number of iterations in the serial reproduction\n",
    "MAX_ITER = 25\n",
    "# Number of epochs per training run\n",
    "EPOCHS = 10\n",
    "\n",
    "for iteration in range(0,MAX_ITER):\n",
    "    # If iteration is seed, train on original target vectors, else, train on y_hat from time t-1\n",
    "    if iteration == 0:\n",
    "        # One-hot encode the labels\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "        mpth = 'model.weights.best.hdf5'\n",
    "        y_hat_test_name = 'y_hat_test_seed'\n",
    "        y_hat_train_name = 'y_hat_train_seed'      \n",
    "    elif iteration > 0:\n",
    "        # Key step: set new targets as y_hat\n",
    "        y_train = y_hat    \n",
    "        mpth = 'model.weights.best.' + 'iter' + str(iteration) + '.hdf5'\n",
    "        y_hat_test_name = 'y_hat_test_' + 'iter' + str(iteration)\n",
    "        y_hat_train_name = 'y_hat_train_' + 'iter' + str(iteration)\n",
    "\n",
    "    # Define the model: a small CNN model (could probably be done outside loop)\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Must define the input shape in the first layer of the neural network\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Take a look at the model summary\n",
    "    # model.summary()\n",
    "\n",
    "    # define optimization and energy parameters\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Save checkpoints\n",
    "    checkpointer = ModelCheckpoint(filepath= 'weights/weights_digits_6/'+ mpth, verbose = 1, save_best_only=False) #True\n",
    "    # Train the model\n",
    "    model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=EPOCHS,\n",
    "             validation_data=(x_valid, y_valid),\n",
    "             callbacks=[checkpointer])\n",
    "\n",
    "    # Load the weights with the best validation accuracy\n",
    "    y_hat = model.predict(x_train) #feed back serial reproduction targets\n",
    "    y_hat_test = model.predict(x_test)\n",
    "    \n",
    "    model.load_weights('weights/weights_digits_6/' + mpth)\n",
    "    # Evaluate the model on test set\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    # Print test accuracy\n",
    "    print('\\n', 'Test accuracy:', score[1])\n",
    "\n",
    "    # Save results for each iteration in the serial reproduction chain\n",
    "    np.save('weights/weights_digits_6/' + y_hat_train_name + DATASET + REGIME + '.npy', y_train)\n",
    "    print('weights/weights_digits_6/' + y_hat_train_name)\n",
    "#     np.save('weights/' + y_hat_test_name + DATASET + REGIME + '.npy', y_test)\n",
    "    np.save('weights/weights_digits_6/' + y_hat_test_name + DATASET + REGIME + '.npy', y_hat_test)\n",
    "    print('weights/weights_digits_6/' + y_hat_test_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "## Visualize some image examples and cnn classifications ##\n",
    "###########################################################\n",
    "\n",
    "rdn = range(25,50)\n",
    "exs = enumerate(rdn)\n",
    "\n",
    "%matplotlib inline\n",
    "# Plot a random sample of 10 test images, their predicted labels and ground truth\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "for i, index in exs:\n",
    "    ax = figure.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(x_test[index]))\n",
    "    predict_index = np.argmax(y_hat_test[index])\n",
    "    true_index = np.argmax(y_test[index])\n",
    "    # Set the title for each image\n",
    "    ax.set_title(\"{} ({})\".format(labels[predict_index], \n",
    "                                  labels[true_index]),\n",
    "                                  color=(\"green\" if predict_index == true_index else \"red\"))\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "## Read prediction softmax layer activations for all test set images, and across all iterations ##\n",
    "##################################################################################################\n",
    "\n",
    "y_hat_arr = np.zeros([y_test.shape[0], MAX_ITER, len(labels)])\n",
    "y_hat_mu = np.zeros([MAX_ITER, len(labels), len(labels)])\n",
    "\n",
    "# Read in all test y_hat for all iterations and store into y_hat_arr\n",
    "for i in range(MAX_ITER):\n",
    "    if i == 0:\n",
    "        y_hat_test_name = 'y_hat_test_seed'\n",
    "    else:\n",
    "        y_hat_test_name = 'y_hat_test_' + 'iter' + str(i)\n",
    "        # Load test set softmax outputs \n",
    "    yh = np.load('weights/' + y_hat_test_name + DATASET + REGIME + '.npy')  \n",
    "    # Store into y_hat_arr\n",
    "    y_hat_arr[:,i,:] = yh\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae7b18",
   "metadata": {},
   "source": [
    "# Compile Softmax Outputs into Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24996134",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "## Read prediction softmax layer activations for all test set images, and all iterations ##\n",
    "###########################################################################################\n",
    "\n",
    "# just including this so I don't need to rerun cell 3 for now!\n",
    "MAX_ITER = 25\n",
    "\n",
    "\n",
    "# Build arrays of dimensions: N training images X L labels X P iterations\n",
    "\n",
    "y_hat_train_arr = np.zeros([y_train.shape[0], len(labels), MAX_ITER])\n",
    "y_hat_test_arr = np.zeros([y_test.shape[0], len(labels), MAX_ITER])\n",
    "\n",
    "for i in range(MAX_ITER):\n",
    "    if i == 0:\n",
    "        y_hat_train_name = 'y_hat_train_seed'\n",
    "        y_hat_test_name = 'y_hat_test_seed'\n",
    "    \n",
    "    else:\n",
    "        y_hat_train_name = 'y_hat_train_' + 'iter' + str(i)\n",
    "        y_hat_test_name = 'y_hat_test_' + 'iter' + str(i)\n",
    "        \n",
    "    # Load test set softmax outputs \n",
    "    yhtr = np.load('weights/weights_digits_1/' + y_hat_train_name + DATASET + REGIME + '.npy')\n",
    "    yhte = np.load('weights/weights_digits_1/' + y_hat_test_name + DATASET + REGIME + '.npy')  \n",
    "\n",
    "    # The first time through, use binary weight vectors to save correct class array\n",
    "    if i == 0:\n",
    "        true_class_tr = np.nonzero(yhtr)[1]\n",
    "        true_class_te = np.nonzero(yhte)[1]\n",
    "        \n",
    "    y_hat_train_arr[:, :, i] = yhtr\n",
    "    y_hat_test_arr[:, :, i] = yhte\n",
    "\n",
    "\n",
    "print(y_hat_train_arr.shape)\n",
    "# (55000, 50, 10)\n",
    "print(y_hat_test_arr.shape)\n",
    "# (10000, 50, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60f4aa",
   "metadata": {},
   "source": [
    "# Visualize N least and most confusing images given rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## Visualize N images ##\n",
    "########################\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.backends.backend_pdf\n",
    "\n",
    "def visualize_N_images(CLASS, NUM_IMAGES, rank):\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(\"Outputs/MNIST_Nx4_Plots/least_confusing_\" + str(labels[CLASS]) + \"s.pdf\")\n",
    "    # NUM_IMAGES softmax outputs least and most confusing images for CLASS\n",
    "    class_data_least_confusing = class_data[rank[:NUM_IMAGES]]\n",
    "    class_data_most_confusing = class_data[rank[-NUM_IMAGES:]]\n",
    "\n",
    "\n",
    "    # extract NUM_IMAGES images of CLASS data from the overall training images\n",
    "    class_images = x_train[np.where(true_class_tr == CLASS)]\n",
    "\n",
    "    images_least_confusing = class_images[rank[:NUM_IMAGES]]\n",
    "    images_most_confusing = class_images[rank[-NUM_IMAGES:]]\n",
    "\n",
    "    # visualize each image\n",
    "    print(str(NUM_IMAGES) + \" least confusing \" + str(labels[CLASS]) + \"s\")\n",
    "    for i in range(NUM_IMAGES):\n",
    "        current = class_data_least_confusing[i]\n",
    "\n",
    "        figure = plt.figure(figsize=(40, 40))\n",
    "        # plot image\n",
    "        ax1 = figure.add_subplot(8, 8, 1, xticks=[], yticks=[])\n",
    "        im1 = ax1.imshow(images_least_confusing[i])\n",
    "        ax1.set_title(\"Image\")\n",
    "\n",
    "        # plot weights graph\n",
    "        ax2 = figure.add_subplot(8, 8, 2)\n",
    "        im2 = ax2.imshow(current.T, cmap='Wistia')\n",
    "\n",
    "        divider = make_axes_locatable(ax2)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "        cbar.ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "        ax2.set(xlabel='Classes', ylabel='Iterations', title='Softmax Outputs')\n",
    "        ax2.set_xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        ax2.set_xticklabels(labels)\n",
    "\n",
    "        # plot correlation graph\n",
    "        corr_arr = np.corrcoef(current.T)\n",
    "\n",
    "        ax3 = figure.add_subplot(8, 8, 3)\n",
    "        im3 = ax3.imshow(corr_arr, cmap='Wistia')\n",
    "        divider = make_axes_locatable(ax3)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "        cbar.ax.set_yticklabels(['0', '1'])\n",
    "        ax3.set_title(\"Correlation Matrix\")\n",
    "\n",
    "        # plot eigenvalues graph\n",
    "        eigs, _ = np.linalg.eig(corr_arr)\n",
    "        num_nonzero = np.count_nonzero(np.around(eigs, 2))\n",
    "\n",
    "        if (num_nonzero == 1):\n",
    "            title_str = \"Sorted Eigenvalues (\" + str(num_nonzero) + \" nonzero)\"\n",
    "        else:\n",
    "            title_str = \"Sorted Eigenvalues (\" + str(num_nonzero) + \" nonzeros)\"\n",
    "\n",
    "        ax4 = figure.add_subplot(8, 8, 4)\n",
    "        im4 = ax4.plot(eigs, marker='o')\n",
    "        ax4.set(xlabel=\"PC Number\", xlim=[0,len(eigs)], ylim=[0,MAX_ITER], title=title_str)\n",
    "        \n",
    "        pdf.savefig(figure, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    pdf.close()\n",
    "\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(\"Outputs/MNIST_Nx4_Plots/most_confusing_\" + str(labels[CLASS]) + \"s.pdf\")\n",
    "    print(str(NUM_IMAGES) + \" most confusing \" + str(labels[CLASS]) + \"s\")\n",
    "    for i in range(NUM_IMAGES):\n",
    "        current = class_data_most_confusing[i]\n",
    "\n",
    "        figure = plt.figure(figsize=(40, 40))\n",
    "        # plot image\n",
    "        ax1 = figure.add_subplot(8, 8, 1, xticks=[], yticks=[])\n",
    "        im1 = ax1.imshow(images_most_confusing[i])\n",
    "        ax1.set_title(\"Image\")\n",
    "\n",
    "        # plot weights graph\n",
    "        ax2 = figure.add_subplot(8, 8, 2)\n",
    "        im2 = ax2.imshow(current.T, cmap='Wistia')\n",
    "\n",
    "        divider = make_axes_locatable(ax2)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "\n",
    "        ax2.set(xlabel='Classes', ylabel='Iterations', title='Softmax Outputs')\n",
    "        ax2.set_xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "        ax2.set_xticklabels(labels)\n",
    "\n",
    "        # plot correlation graph\n",
    "        corr_arr = np.corrcoef(current.T)\n",
    "\n",
    "        ax3 = figure.add_subplot(8, 8, 3)\n",
    "        im3 = ax3.imshow(corr_arr, cmap='Wistia')\n",
    "        divider = make_axes_locatable(ax3)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "        cbar.ax.set_yticklabels(['0', '1'])\n",
    "        ax3.set_title(\"Correlation Matrix\")\n",
    "\n",
    "        # plot eigenvalues graph\n",
    "        eigs, _ = np.linalg.eig(corr_arr)\n",
    "        num_nonzero = np.count_nonzero(np.around(eigs, 2))\n",
    "\n",
    "        if (num_nonzero == 1):\n",
    "            title_str = \"Sorted Eigenvalues (\" + str(num_nonzero) + \" nonzero)\"\n",
    "        else:\n",
    "            title_str = \"Sorted Eigenvalues (\" + str(num_nonzero) + \" nonzeros)\"\n",
    "\n",
    "        ax4 = figure.add_subplot(8, 8, 4)\n",
    "        im4 = ax4.plot(eigs, marker='o')\n",
    "        ax4.set(xlabel=\"PC Number\", xlim=[0,len(eigs)], ylim=[0, MAX_ITER], title=title_str)\n",
    "\n",
    "        \n",
    "        pdf.savefig(figure, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b586cd",
   "metadata": {},
   "source": [
    "# Rank Images by Difference to Binary Seed Vector and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Rank by difference to binary seed vector ##\n",
    "##############################################\n",
    "print(\"Rank method: Binary seed vector difference\")\n",
    "\n",
    "for i in range(10):\n",
    "    # choose which class to analyze\n",
    "    CLASS = i\n",
    "\n",
    "    # take difference of predictionn probability along each row for indicated class\n",
    "    class_data = y_hat_train_arr[np.where(true_class_tr == CLASS), :, :].squeeze()\n",
    "\n",
    "    # calculate differences in weights for each image between true and final iter\n",
    "    diff_arr = class_data[:, CLASS, 0] - class_data[:, CLASS, MAX_ITER - 1]\n",
    "\n",
    "    #rank images by magnitude of difference from correct prediction, default ascending\n",
    "    seed_diff_rank = np.argsort(diff_arr)\n",
    "    \n",
    "    visualize_N_images(i, 100, seed_diff_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed59a4c",
   "metadata": {},
   "source": [
    "# Rank images by entropy and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## Rank by entropy ##\n",
    "#####################\n",
    "print(\"Rank method: Entropy\")\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "for i in range(10):\n",
    "    # choose which class to analyze\n",
    "    CLASS = i\n",
    "\n",
    "    # take difference of predictionn probability along each row for indicated class\n",
    "    class_data = y_hat_train_arr[np.where(true_class_tr == CLASS), :, :].squeeze()\n",
    "\n",
    "    ents = []\n",
    "    for i in range(class_data.shape[0]):\n",
    "        ents.append(entropy(class_data[i, :, :], class_data[0, :, :], base=2, axis=1))\n",
    "    ents = np.asarray(ents)\n",
    "\n",
    "    # rank images by entropy, averaged across rows/images\n",
    "    ents_rank = np.argsort(np.mean(ents, axis=1))\n",
    "    \n",
    "    visualize_N_images(i, 100, ents_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c48be2",
   "metadata": {},
   "source": [
    "# Incorporate monotonicity and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "## Rank by monotonicity * absolute change ##\n",
    "############################################\n",
    "\n",
    "for i in range(10):\n",
    "    # choose which class to analyze\n",
    "    CLASS = i\n",
    "\n",
    "    # take difference of predictionn probability along each row for indicated class\n",
    "    class_data = y_hat_train_arr[np.where(true_class_tr == CLASS), :, :].squeeze()\n",
    "\n",
    "    # calculate element wise differences across iterations to track oscillations\n",
    "    diff_arr = np.diff(class_data, axis=2)[:, CLASS, :]\n",
    "    neg_where = diff_arr < 0\n",
    "    neg_count = neg_where.sum(axis=1)\n",
    "    oscillation_factor = neg_count / neg_where.shape[1]\n",
    "    \n",
    "    # calculate differences in weights for each image between true and final iter\n",
    "    absolute_diff_arr = class_data[:, CLASS, 0] - class_data[:, CLASS, MAX_ITER - 1]\n",
    "    \n",
    "    # multiply absolute difference by oscillation factor\n",
    "    confusion = absolute_diff_arr * oscillation_factor\n",
    "\n",
    "    #rank images by magnitude of difference from correct prediction, default ascending\n",
    "    monotonicity_rank = np.argsort(confusion)\n",
    "\n",
    "    visualize_N_images(i, 100, monotonicity_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08130c6f",
   "metadata": {},
   "source": [
    "# Correlation between methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970047f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "## Seed_diff/Entropy Correlation ##\n",
    "###################################\n",
    "from scipy.stats import spearmanr \n",
    "print(\"seed_diff_rank & entropy: \", spearmanr(seed_diff_rank, ents_rank))\n",
    "print(\"\\nseed_diff_rank & monotonicity: \", spearmanr(seed_diff_rank, monotonicity_rank))\n",
    "print(\"\\nentropy & monotonicity: \", spearmanr(ents_rank, monotonicity_rank))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aac1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "## Correlation between different training runs ##\n",
    "#################################################\n",
    "from scipy.stats import pearsonr \n",
    "import random\n",
    "MAX_ITER = 25\n",
    "\n",
    "y_hat_train_arr_1 = np.zeros([y_train.shape[0], len(labels), MAX_ITER])\n",
    "y_hat_test_arr_1 = np.zeros([y_test.shape[0], len(labels), MAX_ITER])\n",
    "\n",
    "y_hat_train_arr_2 = np.zeros([y_train.shape[0], len(labels), MAX_ITER])\n",
    "y_hat_test_arr_2 = np.zeros([y_test.shape[0], len(labels), MAX_ITER])\n",
    "\n",
    "for i in range(MAX_ITER):\n",
    "    if i == 0:\n",
    "        y_hat_train_name = 'y_hat_train_seed'\n",
    "        y_hat_test_name = 'y_hat_test_seed'\n",
    "    \n",
    "    else:\n",
    "        y_hat_train_name = 'y_hat_train_' + 'iter' + str(i)\n",
    "        y_hat_test_name = 'y_hat_test_' + 'iter' + str(i)\n",
    "        \n",
    "    # Load test set softmax outputs \n",
    "    yhtr_1 = np.load('weights/weights_digits_1/' + y_hat_train_name + DATASET + REGIME + '.npy')\n",
    "    yhte_1 = np.load('weights/weights_digits_1/' + y_hat_test_name + DATASET + REGIME + '.npy')  \n",
    "    \n",
    "    yhtr_2 = np.load('weights/weights_digits_6/' + y_hat_train_name + DATASET + REGIME + '.npy')\n",
    "    yhte_2 = np.load('weights/weights_digits_6/' + y_hat_test_name + DATASET + REGIME + '.npy')  \n",
    "\n",
    "    # The first time through, use binary weight vectors to save correct class array\n",
    "    if i == 0:\n",
    "        true_class_tr_1 = np.nonzero(yhtr_1)[1]\n",
    "        true_class_te_1 = np.nonzero(yhte_1)[1]\n",
    "        \n",
    "        true_class_tr_2 = np.nonzero(yhtr_2)[1]\n",
    "        true_class_te_2 = np.nonzero(yhte_2)[1]\n",
    "        \n",
    "    y_hat_train_arr_1[:, :, i] = yhtr_1\n",
    "    y_hat_test_arr_1[:, :, i] = yhte_1\n",
    "    \n",
    "    y_hat_train_arr_2[:, :, i] = yhtr_2\n",
    "    y_hat_test_arr_2[:, :, i] = yhte_2\n",
    "    \n",
    "# print correlation for each class\n",
    "# build large correlation matrix of N classes X M images X L iters\n",
    "\n",
    "_, counts = np.unique(true_class_tr_1, return_counts=True)\n",
    "max_count = np.amax(counts)\n",
    "\n",
    "correlation_arr = np.zeros([10, max_count, MAX_ITER])\n",
    "\n",
    "for i in range(10):\n",
    "    CLASS = i\n",
    "    print(\"\\nCLASS: \" + str(CLASS) + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # np.where(true_class_tr_1 == CLASS and np.where(true_class_tr_2 == CLASS both \n",
    "    # identify the same indices for the class data in the larger training data\n",
    "    \n",
    "    class_data_1 = y_hat_train_arr_1[np.where(true_class_tr_1 == CLASS), :, :].squeeze()\n",
    "    class_data_2 = y_hat_train_arr_2[np.where(true_class_tr_2 == CLASS), :, :].squeeze()\n",
    "    \n",
    "    diff_arr_1 = class_data_1[:, CLASS, 0] - class_data_1[:, CLASS, MAX_ITER - 1]\n",
    "    diff_arr_2 = class_data_2[:, CLASS, 0] - class_data_2[:, CLASS, MAX_ITER - 1]\n",
    "    seed_diff_rank_1 = np.argsort(diff_arr_1)\n",
    "    seed_diff_rank_2 = np.argsort(diff_arr_2)\n",
    "    \n",
    "    # get the indices of the most confusing 1000 images for each class\n",
    "    confusing_idxs = seed_diff_rank_1[-1000:]\n",
    "    \n",
    "    for j, idx in enumerate(confusing_idxs):\n",
    "        # isolate the softmax outputs for each confusing image \n",
    "        outputs_1 = class_data_1[idx, :, :].T\n",
    "        outputs_2 = class_data_2[idx, :, :].T\n",
    "        \n",
    "        image_corr_sum = 0\n",
    "        for iter_num in range(outputs_1.shape[0]):\n",
    "            weights_1 = outputs_1[iter_num]\n",
    "            weights_2 = outputs_2[iter_num]\n",
    "            corr = np.corrcoef(weights_1, weights_2)[0, 1]\n",
    "            \n",
    "            image_corr_sum += corr\n",
    "            correlation_arr[i, j, iter_num] = corr\n",
    "            \n",
    "        image_corr_avg = image_corr_sum / outputs_1.shape[0]\n",
    "        print(image_corr_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation_arr.shape)\n",
    "for i in range(correlation_arr.shape[0]):\n",
    "    print(np.mean(correlation_arr[i, :1000, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "## Correlation between different training runs where rankings have been shuffled ##\n",
    "###################################################################################\n",
    "\n",
    "correlation_arr_shuffled = np.zeros([10, max_count, MAX_ITER])\n",
    "\n",
    "for i in range(10):\n",
    "    CLASS = i\n",
    "    print(\"\\nCLASS: \" + str(CLASS) + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # np.where(true_class_tr_1 == CLASS and np.where(true_class_tr_2 == CLASS both \n",
    "    # identify the same indices for the class data in the larger training data\n",
    "    \n",
    "    class_data_1 = y_hat_train_arr_1[np.where(true_class_tr_1 == CLASS), :, :].squeeze()\n",
    "    class_data_2 = y_hat_train_arr_2[np.where(true_class_tr_2 == CLASS), :, :].squeeze()\n",
    "    \n",
    "    diff_arr_1 = class_data_1[:, CLASS, 0] - class_data_1[:, CLASS, MAX_ITER - 1]\n",
    "    diff_arr_2 = class_data_2[:, CLASS, 0] - class_data_2[:, CLASS, MAX_ITER - 1]\n",
    "    seed_diff_rank_1 = np.argsort(diff_arr_1)\n",
    "    seed_diff_rank_2 = np.argsort(diff_arr_2)\n",
    "    \n",
    "    # get the indices of the most confusing 1000 images for each class\n",
    "    confusing_idxs = seed_diff_rank_1[-1000:]\n",
    "    \n",
    "    class_data_1_confusing = class_data_1[confusing_idxs]\n",
    "    class_data_2_confusing = class_data_2[confusing_idxs]\n",
    "    \n",
    "    np.random.shuffle(class_data_1_confusing)\n",
    "    np.random.shuffle(class_data_2_confusing)\n",
    "    \n",
    "    for j in range(len(confusing_idxs)):\n",
    "        # isolate the softmax outputs for each confusing image \n",
    "        outputs_1 = class_data_1_confusing[j, :, :].T\n",
    "        outputs_2 = class_data_2_confusing[j, :, :].T\n",
    "        \n",
    "        image_corr_sum = 0\n",
    "        for iter_num in range(outputs_1.shape[0]):\n",
    "            weights_1 = outputs_1[iter_num]\n",
    "            weights_2 = outputs_2[iter_num]\n",
    "            corr = np.corrcoef(weights_1, weights_2)[0, 1]\n",
    "            \n",
    "            image_corr_sum += corr\n",
    "            correlation_arr_shuffled[i, j, iter_num] = corr\n",
    "            \n",
    "        image_corr_avg = image_corr_sum / outputs_1.shape[0]\n",
    "        print(image_corr_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlation_arr_shuffled.shape)\n",
    "for i in range(correlation_arr_shuffled.shape[0]):\n",
    "    print(np.mean(correlation_arr_shuffled[i, :1000, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3dcfb",
   "metadata": {},
   "source": [
    "# Convergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Examine softmax weights for convergence ##\n",
    "#############################################\n",
    "\n",
    "# find norm of difference between each iteration of softmax outputs for all images\n",
    "diff_arr = np.diff(y_hat_train_arr, axis=-1)\n",
    "norm_diff_arr = np.linalg.norm(diff_arr, axis=1)\n",
    "\n",
    "# average across all images\n",
    "avg_change = np.mean(norm_diff_arr, axis=0)\n",
    "\n",
    "# visualize\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "_ = plt.plot(avg_change)\n",
    "_ = plt.title(\"Avg change over iterations\")\n",
    "_ = plt.xlabel(\"Iterations\")\n",
    "_ = plt.ylabel(\"Norm of softmax output differences by iteration\")\n",
    "_ = plt.xticks(np.arange(0, 50, step=1))\n",
    "_ = plt.xlim(0, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d98a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Examine softmax weights for convergence ##\n",
    "#############################################\n",
    "\n",
    "# find norm of difference between each iteration and the seed vector\n",
    "seed_vector_arr =  y_hat_train_arr[:, :, 0]\n",
    "diff_arr = y_hat_train_arr - seed_vector_arr[:, :, None]\n",
    "norm_diff_arr = np.linalg.norm(diff_arr, axis=1)\n",
    "\n",
    "# average across all images\n",
    "avg_change = np.mean(norm_diff_arr, axis=0)\n",
    "\n",
    "# visualize\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "_ = plt.plot(avg_change)\n",
    "_ = plt.title(\"Avg change over iterations\")\n",
    "_ = plt.xlabel(\"Iterations\")\n",
    "_ = plt.ylabel(\"Norm of softmax output differences from seed vector\")\n",
    "_ = plt.xticks(np.arange(0, 25, step=1))\n",
    "_ = plt.xlim(0, 25)\n",
    "_ = plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eaa74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## Convergence by digit ##\n",
    "##########################\n",
    "import matplotlib.backends.backend_pdf\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"Outputs/convergence_by_class.pdf\")\n",
    "for i in range(y_hat_train_arr.shape[1]):\n",
    "    CLASS = i\n",
    "    class_data = y_hat_train_arr[np.where(true_class_tr == CLASS), :, :].squeeze()\n",
    "    \n",
    "    seed_vector_arr =  class_data[:, :, 0]\n",
    "    diff_arr = class_data - seed_vector_arr[:, :, None]\n",
    "    norm_diff_arr = np.linalg.norm(diff_arr, axis=1)\n",
    "    \n",
    "    avg_change_by_class = np.mean(norm_diff_arr, axis=0)\n",
    "    \n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    _ = plt.plot(avg_change_by_class)\n",
    "    _ = plt.title(\"Avg change over iterations, class = \" + str(CLASS))\n",
    "    _ = plt.xlabel(\"Iterations\")\n",
    "    _ = plt.ylabel(\"Norm of softmax output differences from seed vector\")\n",
    "    _ = plt.xticks(np.arange(0, 50, step=1))\n",
    "    _ = plt.xlim(0, 50)\n",
    "    _ = plt.ylim(0,)\n",
    "    \n",
    "    pdf.savefig(figure, bbox_inches='tight')\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## Convergence over 4 rounds of 50 iteratinos, AVG + digits ##\n",
    "##############################################################\n",
    "\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"Outputs/convergence_50_iterations_avgs_and_classes.pdf\")\n",
    "for a in range(2, 6):\n",
    "    \n",
    "    # load in correct weights\n",
    "    y_hat_train_arr = np.zeros([y_train.shape[0], len(labels), MAX_ITER])\n",
    "    y_hat_test_arr = np.zeros([y_test.shape[0], len(labels), MAX_ITER])\n",
    "\n",
    "    for i in range(MAX_ITER):\n",
    "        if i == 0:\n",
    "            y_hat_train_name = 'y_hat_train_seed'\n",
    "            y_hat_test_name = 'y_hat_test_seed'\n",
    "\n",
    "        else:\n",
    "            y_hat_train_name = 'y_hat_train_' + 'iter' + str(i)\n",
    "            y_hat_test_name = 'y_hat_test_' + 'iter' + str(i)\n",
    "\n",
    "        # Load test set softmax outputs \n",
    "        yhtr = np.load('weights/weights_fashion_1/' + y_hat_train_name + DATASET + REGIME + '.npy')\n",
    "        yhte = np.load('weights/weights_fashion_1/' + y_hat_test_name + DATASET + REGIME + '.npy')  \n",
    "\n",
    "        # The first time through, use binary weight vectors to save correct class array\n",
    "        if i == 0:\n",
    "            true_class_tr = np.nonzero(yhtr)[1]\n",
    "            true_class_te = np.nonzero(yhte)[1]\n",
    "\n",
    "        y_hat_train_arr[:, :, i] = yhtr\n",
    "        y_hat_test_arr[:, :, i] = yhte\n",
    "    \n",
    "    \n",
    "    \n",
    "    # find norm of difference between each iteration and the seed vector\n",
    "    seed_vector_arr =  y_hat_train_arr[:, :, 0]\n",
    "    diff_arr = y_hat_train_arr - seed_vector_arr[:, :, None]\n",
    "    norm_diff_arr = np.linalg.norm(diff_arr, axis=1)\n",
    "\n",
    "    # average across all images\n",
    "    avg_change = np.mean(norm_diff_arr, axis=0)\n",
    "\n",
    "    # visualize\n",
    "    figure = plt.figure(figsize=(20, 8))\n",
    "    _ = plt.plot(avg_change, linewidth=5, label=\"average\")\n",
    "    \n",
    "    for b in range(y_hat_train_arr.shape[1]):\n",
    "        CLASS = b\n",
    "        class_data = y_hat_train_arr[np.where(true_class_tr == CLASS), :, :].squeeze()\n",
    "\n",
    "        seed_vector_arr =  class_data[:, :, 0]\n",
    "        diff_arr = class_data - seed_vector_arr[:, :, None]\n",
    "        norm_diff_arr = np.linalg.norm(diff_arr, axis=1)\n",
    "\n",
    "        avg_change_by_class = np.mean(norm_diff_arr, axis=0)\n",
    "        _ = plt.plot(avg_change_by_class, label=labels[b])\n",
    "    \n",
    "    \n",
    "    \n",
    "    _ = plt.title(\"Avg change over iterations\")\n",
    "    _ = plt.xlabel(\"Iterations\")\n",
    "    _ = plt.ylabel(\"Norm of softmax output differences from seed vector\")\n",
    "    _ = plt.xticks(np.arange(0, 50, step=1))\n",
    "    _ = plt.xlim(0, 50)\n",
    "    _ = plt.ylim(0,)\n",
    "    _ = plt.legend(loc='upper right')\n",
    "    _ = plt.show()\n",
    "    pdf.savefig(figure)\n",
    "    \n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fea1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "## Convergence over 4 rounds of 50 iteratinos, only AVGs ##\n",
    "###########################################################\n",
    "\n",
    "\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(\"Outputs/convergence_50_iterations_avgs.pdf\")\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "for a in range(2, 6):\n",
    "    \n",
    "    # load in correct weights\n",
    "    y_hat_train_arr = np.zeros([y_train.shape[0], len(labels), MAX_ITER])\n",
    "    y_hat_test_arr = np.zeros([y_test.shape[0], len(labels), MAX_ITER])\n",
    "\n",
    "    for i in range(MAX_ITER):\n",
    "        if i == 0:\n",
    "            y_hat_train_name = 'y_hat_train_seed'\n",
    "            y_hat_test_name = 'y_hat_test_seed'\n",
    "\n",
    "        else:\n",
    "            y_hat_train_name = 'y_hat_train_' + 'iter' + str(i)\n",
    "            y_hat_test_name = 'y_hat_test_' + 'iter' + str(i)\n",
    "\n",
    "        # Load test set softmax outputs \n",
    "        yhtr = np.load('weights' + str(a) + '/' + y_hat_train_name + DATASET + REGIME + '.npy')\n",
    "        yhte = np.load('weights' + str(a) + '/' + y_hat_test_name + DATASET + REGIME + '.npy')  \n",
    "\n",
    "        # The first time through, use binary weight vectors to save correct class array\n",
    "        if i == 0:\n",
    "            true_class_tr = np.nonzero(yhtr)[1]\n",
    "            true_class_te = np.nonzero(yhte)[1]\n",
    "\n",
    "        y_hat_train_arr[:, :, i] = yhtr\n",
    "        y_hat_test_arr[:, :, i] = yhte\n",
    "    \n",
    "    \n",
    "    \n",
    "    # find norm of difference between each iteration and the seed vector\n",
    "    seed_vector_arr =  y_hat_train_arr[:, :, 0]\n",
    "    diff_arr = y_hat_train_arr - seed_vector_arr[:, :, None]\n",
    "    norm_diff_arr = np.linalg.norm(diff_arr, axis=1)\n",
    "\n",
    "    # average across all images\n",
    "    avg_change = np.mean(norm_diff_arr, axis=0)\n",
    "\n",
    "    # visualize\n",
    "\n",
    "    _ = plt.plot(avg_change, label=(\"iteration: \" + str(a - 2)))\n",
    "\n",
    "\n",
    "_ = plt.title(\"Avg change over iterations\")\n",
    "_ = plt.xlabel(\"Iterations\")\n",
    "_ = plt.ylabel(\"Norm of softmax output differences from seed vector\")\n",
    "_ = plt.xticks(np.arange(0, 50, step=1))\n",
    "_ = plt.xlim(0, 50)\n",
    "_ = plt.ylim(0,)\n",
    "_ = plt.legend(loc='upper left')\n",
    "_ = plt.show()\n",
    "pdf.savefig(figure)\n",
    "    \n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff1ed2",
   "metadata": {},
   "source": [
    "# Scratchwork/incomplete cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1212aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function/ Custom Loss function\n",
    "\n",
    "# vector of length N images, higher weights for more confusable images\n",
    "confusion_rank = []\n",
    "def wrapper(confusion_rank):\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # calculate loss between y_true and y_pred\n",
    "    # element wise multiply the loss by confusion rank\n",
    "    \n",
    "    # why take the mean? \n",
    "    \n",
    "    return loss\n",
    "return custom_loss\n",
    "\n",
    "\n",
    "\n",
    "# define weights based on serial reproduction results, for each input image --- contains larger weights for images that get confused with the wrong class following serial reproduction.\n",
    "weights = <a vector of weights the length of the inputs y_true and y_pred>\n",
    "# a wrapper function that weighs the loss using the weights vector before averaging the squared error.\n",
    "def wrapper(weights):\n",
    "def custom_loss_1(y_true, y_pred):\n",
    "  diff = math_ops.squared_difference(y_pred, y_true)  #squared difference\n",
    "  loss = diff * weights # elementwise multiply\n",
    "  loss = K.mean(diff, axis=-1) #mean\n",
    "  return loss\n",
    "return custom_loss_1 (edited) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7470ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select digit/class\n",
    "for CATEGORY in range(10):\n",
    "    \n",
    "    # select '1' for correct predictions, '0' for incorrect predictions, or '2' for all\n",
    "    CORRECT = 2\n",
    "    I = 0\n",
    "\n",
    "    # get correct/wrong classifications in final iteration\n",
    "    final = y_hat_arr[:,MAX_ITER-1,:]\n",
    "    # store binarized predictions from final iteration\n",
    "    final_bin = np.zeros(final.shape)\n",
    "    \n",
    "    for i in range(final.shape[0]):\n",
    "        final_bin[i,np.argmax(final[i,:])] = 1.0\n",
    "    # incorrect ones\n",
    "    idx = final_bin[:,CATEGORY]\n",
    "\n",
    "    # 0s indices\n",
    "    idx0 = np.argwhere(y_hat_arr[:,0,CATEGORY] == 1.0)\n",
    "    print(idx0.shape)\n",
    "\n",
    "    if CORRECT == 1:\n",
    "        colors = plt.cm.winter(np.linspace(0,1,MAX_ITER))\n",
    "        idx1 = [idx for idx, v in enumerate(idx) if v == 1.0]\n",
    "        c = np.in1d(idx0,idx1)\n",
    "        idx0 = idx0[c]        \n",
    "    elif CORRECT == 0:\n",
    "        colors = plt.cm.cool(np.linspace(0,1,MAX_ITER))\n",
    "        idx1 = [idx for idx, v in enumerate(idx) if v != 1.0]\n",
    "        c = np.in1d(idx0,idx1)\n",
    "        idx0 = idx0[c]\n",
    "        \n",
    "    print(idx0.shape)\n",
    "\n",
    "    ##########################################################\n",
    "    ## Draw the plot with the results across all iterations ##\n",
    "    ##########################################################\n",
    "    \n",
    "    MRK = 100\n",
    "    LNW = 5\n",
    "    figure = plt.figure(figsize=(40, 40))\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(0,MAX_ITER-1):\n",
    "        plt.plot(range(10), np.mean(y_hat_arr[idx0,i,:],0).flatten(), \"o:\", color=colors[i], linewidth=LNW, markersize=MRK)\n",
    "    plt.xticks(range(len(labels)),labels)\n",
    "    plt.xticks(rotation = 90) \n",
    "    plt.xticks(fontsize=100)\n",
    "    plt.yticks(fontsize=100)\n",
    "    plt.xlabel('class', fontsize=120)\n",
    "    plt.ylabel('Probability of class (average)', fontsize=120)\n",
    "    plt.title('Average predictions by iteration: ' + labels[CATEGORY],fontsize=100)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
